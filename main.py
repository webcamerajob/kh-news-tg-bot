import os
import sys
import json
import logging
import time
import requests
import main  # –¢–≤–æ–π main.py —Å —É–º–Ω—ã–º —Ñ–∏–ª—å—Ç—Ä–æ–º –∫–∞—Ä—Ç–∏–Ω–æ–∫

# --- –°–ü–ò–°–û–ö –ú–û–î–ï–õ–ï–ô ---
AI_MODELS = [
    "meta-llama/llama-3.3-70b-instruct:free",
    "google/gemini-2.0-flash-exp:free",
    "deepseek/deepseek-r1-distill-llama-70b:free",
]

OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")

# --- –ü–†–Ø–ú–û–ô GOOGLE –ü–ï–†–ï–í–û–î (GTX) ---
def direct_google_translate(text: str, to_lang: str = "ru") -> str:
    """
    –ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ Google API.
    –†–µ–∂–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –∫—É—Å–∫–∏ –ø–æ 1800 —Å–∏–º–≤–æ–ª–æ–≤, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –æ—à–∏–±–æ–∫ –¥–ª–∏–Ω—ã URL.
    """
    if not text: return ""
    
    chunks = []
    current_chunk = ""
    # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å—Ç—Ä–æ–∫–∞–º, —á—Ç–æ–±—ã –Ω–µ —Ä–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    for paragraph in text.split('\n'):
        # –ï—Å–ª–∏ —á–∞–Ω–∫ –ø–µ—Ä–µ–ø–æ–ª–Ω—è–µ—Ç—Å—è, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ –∏ –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π
        if len(current_chunk) + len(paragraph) < 1800:
            current_chunk += paragraph + "\n"
        else:
            chunks.append(current_chunk)
            current_chunk = paragraph + "\n"
    if current_chunk: chunks.append(current_chunk)
    
    translated_parts = []
    url = "https://translate.googleapis.com/translate_a/single"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"}
    
    for chunk in chunks:
        if not chunk.strip():
            translated_parts.append("")
            continue
        try:
            params = {"client": "gtx", "sl": "en", "tl": to_lang, "dt": "t", "q": chunk.strip()}
            r = requests.get(url, params=params, headers=headers, timeout=5)
            if r.status_code == 200:
                data = r.json()
                text_part = "".join([item[0] for item in data[0] if item and item[0]])
                translated_parts.append(text_part)
            else:
                # –ï—Å–ª–∏ —Å–±–æ–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –∫—É—Å–∫–∞, —á—Ç–æ–±—ã –Ω–µ —Ç–µ—Ä—è—Ç—å —Ç–µ–∫—Å—Ç
                translated_parts.append(chunk)
            time.sleep(0.2)
        except Exception:
            translated_parts.append(chunk)

    return "\n".join(translated_parts)

# --- –£–¢–ò–õ–ò–¢–´ ---
def format_paragraphs(text: str) -> str:
    paragraphs = [p.strip() for p in text.replace('\r', '').split('\n') if p.strip()]
    return "\n\n".join(paragraphs)

def strip_ai_chatter(text: str) -> str:
    # –£–¥–∞–ª—è–µ—Ç –≤—Å—Ç—É–ø–ª–µ–Ω–∏—è —Ç–∏–ø–∞ "Here is the summary"
    bad_prefixes = ["Here is", "The article", "Summary:", "Cleaned text:"]
    for prefix in bad_prefixes:
        if text.lower().startswith(prefix.lower()):
            parts = text.split('\n', 1)
            if len(parts) > 1: return parts[1].strip()
    return text

# --- –ì–õ–ê–í–ù–ê–Ø –õ–û–ì–ò–ö–ê (AI + CONTEXT) ---
def ai_clean_and_then_translate(text: str, to_lang: str = "ru", provider: str = "ai") -> str:
    if not text or not text.strip(): return ""

    # 1. –ü–†–û–í–ï–†–Ø–ï–ú –†–ê–ó–î–ï–õ–ò–¢–ï–õ–¨ –ò–ó MAIN.PY
    # main.py –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –Ω–∞–º —Å—Ç—Ä–æ–∫—É –≤–∏–¥–∞: "–ó–∞–≥–æ–ª–æ–≤–æ–∫ ||| –¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏"
    DELIMITER = " ||| "
    title_part = ""
    body_part = text
    has_delimiter = False

    if DELIMITER in text:
        parts = text.split(DELIMITER, 1)
        title_part = parts[0]
        body_part = parts[1]
        has_delimiter = True

    # 2. –ï–°–õ–ò –¢–ï–ö–°–¢ –ö–û–†–û–¢–ö–ò–ô - –ù–ï –¢–†–ê–¢–ò–ú –í–†–ï–ú–Ø –ù–ê –ò–ò
    # –ü–µ—Ä–µ–≤–æ–¥–∏–º —Å—Ä–∞–∑—É –≤—Å—é —Å–∫–ª–µ–π–∫—É, –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—Å—è
    if len(body_part) < 500:
        return direct_google_translate(text, to_lang)
    
    # 3. –ï–°–õ–ò –¢–ï–ö–°–¢ –î–õ–ò–ù–ù–´–ô - –ß–ò–°–¢–ò–ú –¢–û–õ–¨–ö–û –¢–ï–õ–û (BODY)
    clean_body_english = body_part

    if OPENROUTER_API_KEY: 
        logging.info("‚è≥ –ü–∞—É–∑–∞ 5 —Å–µ–∫ –ø–µ—Ä–µ–¥ –ò–ò...")
        time.sleep(5) 
        logging.info(f"ü§ñ [AI] –ì–ª—É–±–æ–∫–∞—è —á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞...")

        # –ü—Ä–æ–º–ø—Ç: —É–¥–∞–ª—è–µ–º –≤–æ–¥—É –∏ –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è —Ü–∏—Ç–∞—Ç—ã
        prompt = (
            f"You are a ruthless news editor.\n"
            f"INPUT: Raw news text.\n"
            f"OUTPUT: A cleaned-up version of the story in ENGLISH.\n\n"
            "STRICT EDITING RULES:\n"
            "1. CONSOLIDATE NARRATIVE & SPEECH: If the author states a fact, and then a speaker repeats the same meaning, DELETE the speaker's part.\n"
            "2. KEEP UNIQUE DETAILS: Only keep quotes if they add numbers, dates, or emotion.\n"
            "3. REMOVE FLUFF: Delete ads and diplomatic praise.\n"
            "4. NO META-TALK: Start with the story immediately.\n\n"
            f"RAW TEXT:\n{body_part[:15000]}"
        )

        ai_result = ""
        for model in AI_MODELS:
            try:
                response = requests.post(
                    url="https://openrouter.ai/api/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
                        "Content-Type": "application/json",
                        "HTTP-Referer": "https://github.com/parser-bot",
                        "X-Title": "NewsBot",
                    },
                    data=json.dumps({
                        "model": model,
                        "messages": [{"role": "user", "content": prompt}],
                        "temperature": 0.3
                    }),
                    timeout=60
                )
                if response.status_code == 200:
                    result = response.json()
                    if 'choices' in result and result['choices']:
                        ai_result = result['choices'][0]['message']['content'].strip()
                        logging.info(f"‚úÖ [AI] –ß–∏—Å—Ç–∫–∞ —É—Å–ø–µ—à–Ω–∞ ({model}).")
                        break
                elif response.status_code == 429:
                    time.sleep(2)
            except Exception: continue

        # –ï—Å–ª–∏ –ò–ò –æ—Ç–≤–µ—Ç–∏–ª - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ, –∏–Ω–∞—á–µ –æ—Å—Ç–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª
        if ai_result:
            clean_body_english = strip_ai_chatter(ai_result)
        else:
            clean_body_english = body_part 
    
    # 4. –°–ö–õ–ï–ò–í–ê–ï–ú –û–ë–†–ê–¢–ù–û –î–õ–Ø –ü–ï–†–ï–í–û–î–ê
    # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ + ||| + –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    if has_delimiter:
        final_text_to_translate = f"{title_part}{DELIMITER}{clean_body_english}"
    else:
        final_text_to_translate = clean_body_english

    # 5. –ü–ï–†–ï–í–û–î–ò–ú –°–ö–õ–ï–ô–ö–£
    logging.info(f"üåç [Google Direct] –ü–µ—Ä–µ–≤–æ–¥ (–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π)...")
    translated_text = direct_google_translate(final_text_to_translate, to_lang)
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç—Ä–æ–∫—É —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º. main.py —Å–∞–º –µ—ë —Ä–∞–∑—Ä–µ–∂–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç –∂–∏—Ä–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫.
    return translated_text

if __name__ == "__main__":
    # –ú–æ–Ω—Ç–∏—Ä—É–µ–º –Ω–∞—à—É —Ñ—É–Ω–∫—Ü–∏—é –≤ main.py
    main.translate_text = ai_clean_and_then_translate
    main.main()
