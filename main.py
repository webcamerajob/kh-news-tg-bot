import argparse
import logging
import json
import hashlib
import time
import re
import os
import shutil
import html
import fcntl
import subprocess
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Dict, List, Optional, Set

# –î–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º requests (—Å—Ç–∞–±–∏–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å Google GTX)
import requests 
from bs4 import BeautifulSoup
# –î–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º curl_cffi —Å –ø—Ä–æ—Ñ–∏–ª–µ–º Safari (—á—Ç–æ–±—ã —Å–∞–π—Ç –Ω–µ –±–∞–Ω–∏–ª)
from curl_cffi import requests as cffi_requests, CurlHttpVersion

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO, 
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger(__name__)

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
OUTPUT_DIR = Path("articles")
CATALOG_PATH = OUTPUT_DIR / "catalog.json"
MAX_RETRIES = 3
BASE_DELAY = 1.0
MAX_POSTED_RECORDS = 100 
FETCH_DEPTH = 100 

OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")

AI_MODELS = [
    "meta-llama/llama-3.3-70b-instruct:free",
    "google/gemini-2.0-flash-exp:free",
    "deepseek/deepseek-r1-distill-llama-70b:free",
]

# --- –ù–ê–°–¢–†–û–ô–ö–ò –°–ï–¢–ò (PARSER) ---
SCRAPER = cffi_requests.Session(impersonate="safari15_5")
SCRAPER.headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.5 Safari/605.1.15",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9",
    "Referer": "https://www.google.com/"
}
SCRAPER_TIMEOUT = 60 
BAD_RE = re.compile(r"[\u200b-\u200f\uFEFF\u200E\u00A0]")

# --- –ë–õ–û–ö 1: –ü–ï–†–ï–í–û–î –ò –ò–ò ---

def direct_google_translate(text: str, to_lang: str = "ru") -> str:
    """–ü–µ—Ä–µ–≤–æ–¥ —á–µ—Ä–µ–∑ Google GTX —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
    if not text: return ""
    chunks = []
    current_chunk = ""
    for paragraph in text.split('\n'):
        if len(current_chunk) + len(paragraph) < 1800:
            current_chunk += paragraph + "\n"
        else:
            chunks.append(current_chunk)
            current_chunk = paragraph + "\n"
    if current_chunk: chunks.append(current_chunk)
    
    translated_parts = []
    url = "https://translate.googleapis.com/translate_a/single"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"}
    
    logger.info(f"üåç –ü–µ—Ä–µ–≤–æ–¥: —Ä–∞–∑–±–∏—Ç–æ –Ω–∞ {len(chunks)} —á–∞—Å—Ç–µ–π.")
    for i, chunk in enumerate(chunks):
        if not chunk.strip():
            translated_parts.append("")
            continue
        try:
            params = {"client": "gtx", "sl": "en", "tl": to_lang, "dt": "t", "q": chunk.strip()}
            r = requests.get(url, params=params, headers=headers, timeout=10)
            if r.status_code == 200:
                data = r.json()
                text_part = "".join([item[0] for item in data[0] if item and item[0]])
                translated_parts.append(text_part)
                logger.info(f"   - –ß–∞—Å—Ç—å {i+1} –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–∞.")
            else:
                translated_parts.append(chunk)
            time.sleep(0.3)
        except Exception as e:
            logger.error(f"   - –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ —á–∞—Å—Ç–∏ {i+1}: {e}")
            translated_parts.append(chunk)
    return "\n".join(translated_parts)

def strip_ai_chatter(text: str) -> str:
    bad_prefixes = ["Here is", "The article", "Summary:", "Cleaned text:"]
    for prefix in bad_prefixes:
        if text.lower().startswith(prefix.lower()):
            parts = text.split('\n', 1)
            if len(parts) > 1: return parts[1].strip()
    return text

def smart_process_and_translate(title: str, body: str, lang: str) -> (str, str):
    clean_body = body
    if OPENROUTER_API_KEY and len(body) > 500:
        logger.info("ü§ñ AI: –ù–∞—á–∏–Ω–∞–µ–º —á–∏—Å—Ç–∫—É —Ç–µ–∫—Å—Ç–∞...")
        time.sleep(3)
        prompt = (
            f"You are a ruthless news editor.\nINPUT: Raw news text.\nOUTPUT: A cleaned-up version of the story in ENGLISH.\n\n"
            "STRICT EDITING RULES:\n1. CONSOLIDATE NARRATIVE & SPEECH: If a fact is repeated, delete the repetition.\n"
            "2. REMOVE ADS: Delete all fluff and promotional links.\n"
            f"RAW TEXT:\n{body[:15000]}"
        )
        ai_result = ""
        for model in AI_MODELS:
            try:
                logger.info(f"   - –ü—Ä–æ–±—É–µ–º –º–æ–¥–µ–ª—å: {model}")
                response = requests.post(
                    url="https://openrouter.ai/api/v1/chat/completions",
                    headers={"Authorization": f"Bearer {OPENROUTER_API_KEY}", "Content-Type": "application/json"},
                    data=json.dumps({"model": model, "messages": [{"role": "user", "content": prompt}], "temperature": 0.3}),
                    timeout=60
                )
                if response.status_code == 200:
                    result = response.json()
                    ai_result = result['choices'][0]['message']['content'].strip()
                    logger.info("   ‚úÖ AI —É—Å–ø–µ—à–Ω–æ –ø–æ—á–∏—Å—Ç–∏–ª —Ç–µ–∫—Å—Ç.")
                    break
            except Exception as e: 
                logger.warning(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ AI ({model}): {e}")
                continue
        if ai_result: clean_body = strip_ai_chatter(ai_result)

    DELIMITER = " ||| "
    combined_text = f"{title}{DELIMITER}{clean_body}"
    translated_full = direct_google_translate(combined_text, lang)
    final_title, final_text = title, clean_body
    if translated_full:
        if DELIMITER in translated_full:
            parts = translated_full.split(DELIMITER, 1)
            final_title, final_text = parts[0].strip(), parts[1].strip()
    return final_title, final_text

# --- –ë–õ–û–ö 2: –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ---

def cleanup_old_articles(posted_ids_path: Path, articles_dir: Path):
    if not posted_ids_path.is_file() or not articles_dir.is_dir(): return
    try:
        with open(posted_ids_path, 'r', encoding='utf-8') as f:
            all_posted = json.load(f)
            ids_to_keep = set(str(x) for x in all_posted[-MAX_POSTED_RECORDS:])
        cleaned = 0
        for f in articles_dir.iterdir():
            if f.is_dir() and f.name.split('_', 1)[0] not in ids_to_keep:
                shutil.rmtree(f)
                cleaned += 1
        if cleaned > 0: logger.info(f"üßπ –û—á–∏—â–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö —Å—Ç–∞—Ç–µ–π: {cleaned}")
    except Exception as e: logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {e}")

def sanitize_text(text: str) -> str:
    if not text: return ""
    text = html.unescape(text)
    text = re.sub(r'<[^>]+>', '', text)
    return re.sub(r'\n{3,}', '\n\n', text).strip()

def load_posted_ids(state_file_path: Path) -> Set[str]:
    try:
        if state_file_path.exists():
            with open(state_file_path, 'r', encoding='utf-8') as f:
                fcntl.flock(f, fcntl.LOCK_SH)
                return {str(item) for item in json.load(f)}
        return set()
    except Exception: return set()

def load_stopwords(file_path: Optional[Path]) -> List[str]:
    if not file_path or not file_path.exists(): return []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return [line.strip().lower() for line in f if line.strip()]
    except Exception: return []

# --- –ë–õ–û–ö 3: –£–ú–ù–´–ô –ü–û–ò–°–ö –ö–ê–†–¢–ò–ù–û–ö –ò –í–ò–î–ï–û ---

def apply_watermark_to_image(img_path: Path, watermark_path: str = "watermark.png"):
    """–ù–∞–∫–ª–∞–¥—ã–≤–∞–µ—Ç –≤–æ—Ç–µ—Ä–º–∞—Ä–∫—É (35% —à–∏—Ä–∏–Ω—ã) –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"""
    if not os.path.exists(watermark_path) or not img_path.exists(): return
    temp_out = img_path.with_name(f"wm_{img_path.name}")
    
    # –†–∞—Å—á–µ—Ç: 35% —à–∏—Ä–∏–Ω—ã, –ø—Ä–∞–≤—ã–π –≤–µ—Ä—Ö–Ω–∏–π —É–≥–æ–ª (–æ—Ç—Å—Ç—É–ø 10px)
    cmd = [
        "ffmpeg", "-y", "-i", str(img_path), "-i", watermark_path,
        "-filter_complex", "[1:v][0:v]scale2ref=iw*0.35:-1[wm][img];[img][wm]overlay=W-w-10:10",
        "-q:v", "2", str(temp_out)
    ]
    try:
        subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)
        os.replace(temp_out, img_path)
    except Exception as e: logger.error(f"   ‚ùå FFmpeg Image Error ({img_path.name}): {e}")

def process_video_logic(video_url, watermark_path="watermark.png"):
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ (360p + –≤–æ—Ç–µ—Ä–º–∞—Ä–∫–∞)"""
    if not video_url: return None
    logger.info(f"üé¨ –í–∏–¥–µ–æ: –ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ {video_url}. –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É...")
    ts = int(time.time())
    raw, final = Path(f"raw_{ts}.mp4"), Path(f"video_{ts}.mp4")
    session = cffi_requests.Session(impersonate="chrome120")
    try:
        # 1. –ó–∞–ø—Ä–æ—Å –∫ Loader.to
        resp = session.get("https://loader.to/ajax/download.php", params={"format": "360", "url": video_url}, timeout=15)
        task_id = resp.json().get("id")
        
        # 2. –û–∂–∏–¥–∞–Ω–∏–µ
        download_url = None
        for i in range(25):
            time.sleep(3)
            status = session.get("https://loader.to/ajax/progress.php", params={"id": task_id}).json()
            logger.info(f"   - –û–∂–∏–¥–∞–Ω–∏–µ –≤–∏–¥–µ–æ ({i+1}/25): {status.get('text')}")
            if status.get("success") == 1:
                download_url = status.get("download_url")
                break
        
        if not download_url: 
            logger.error("   ‚ùå –í–∏–¥–µ–æ –Ω–µ –±—ã–ª–æ –ø–æ–ª—É—á–µ–Ω–æ –æ—Ç Loader.to")
            return None

        # 3. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ
        logger.info("   - –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞...")
        with session.get(download_url, stream=True) as r:
            with open(raw, 'wb') as f:
                for chunk in r.iter_content(8192): f.write(chunk)
        
        # 4. FFmpeg
        logger.info("   - –†–µ–Ω–¥–µ—Ä–∏–Ω–≥ –≤–æ—Ç–µ—Ä–º–∞—Ä–∫–∏ (360p)...")
        cmd = [
            "ffmpeg", "-y", "-i", str(raw), "-i", watermark_path,
            "-filter_complex", "[1:v][0:v]scale2ref=iw*0.35:-1[wm][vid];[vid][wm]overlay=W-w-10:10",
            "-c:v", "libx264", "-preset", "superfast", "-crf", "28", "-c:a", "copy", str(final)
        ]
        subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)
        if raw.exists(): raw.unlink()
        logger.info(f"   ‚úÖ –í–∏–¥–µ–æ –≥–æ—Ç–æ–≤–æ: {final}")
        return str(final)
    except Exception as e:
        logger.error(f"   ‚ùå –û—à–∏–±–∫–∞ –≤–∏–¥–µ–æ: {e}")
        if raw.exists(): raw.unlink()
        return None

def extract_img_url(img_tag: Any) -> Optional[str]:
    width_attr = img_tag.get("width")
    if width_attr and width_attr.isdigit() and int(width_attr) < 400: return None
    attrs = ["data-orig-file", "data-large-file", "data-src", "data-lazy-src", "src"]
    for attr in attrs:
        if val := img_tag.get(attr):
            clean_val = val.split()[0].split(',')[0].split('?')[0]
            return clean_val
    return None

def save_image(url, folder):
    folder.mkdir(parents=True, exist_ok=True)
    fn = hashlib.md5(url.encode()).hexdigest() + ".jpg"
    dest = folder / fn
    try:
        dest.write_bytes(SCRAPER.get(url, timeout=SCRAPER_TIMEOUT).content)
        # –ù–∞–∫–ª–∞–¥—ã–≤–∞–µ–º –≤–æ—Ç–µ—Ä–º–∞—Ä–∫—É —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        apply_watermark_to_image(dest)
        logger.info(f"   üñºÔ∏è –§–æ—Ç–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∏ –≤–æ—Ç–µ—Ä–º–∞—Ä–∫–Ω—É—Ç–æ: {fn}")
        return str(dest)
    except Exception as e:
        logger.error(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–æ—Ç–æ {url}: {e}")
        return None

# --- –ë–õ–û–ö 4: API –ò –ü–ê–†–°–ò–ù–ì ---

def fetch_cat_id(url, slug):
    r = SCRAPER.get(f"{url}/wp-json/wp/v2/categories?slug={slug}", timeout=SCRAPER_TIMEOUT)
    r.raise_for_status()
    return r.json()[0]["id"]

def fetch_posts(url, cid, limit):
    logger.info(f"üì° –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º {limit} –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π –∏–∑ API...") 
    try:
        r = SCRAPER.get(f"{url}/wp-json/wp/v2/posts?categories={cid}&per_page={limit}&_embed", timeout=SCRAPER_TIMEOUT)
        r.raise_for_status()
        posts = r.json()
        logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(posts)} –ø–æ—Å—Ç–æ–≤.")
        return posts
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ API: {e}")
        return []
        
def parse_and_save(post, lang, stopwords):
    time.sleep(2)
    aid, slug, link = str(post["id"]), post["slug"], post.get("link")
    raw_title = BeautifulSoup(post["title"]["rendered"], "html.parser").get_text(strip=True)
    title = sanitize_text(raw_title)

    logger.info(f"üìñ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏ ID {aid}: {title[:50]}...")

    if stopwords and any(ph in title.lower() for ph in stopwords):
        logger.info(f"   üö´ –ü—Ä–æ–ø—É—Å–∫: –°—Ç–æ–ø-—Å–ª–æ–≤–æ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ.")
        return None

    try:
        html_txt = SCRAPER.get(link, timeout=SCRAPER_TIMEOUT).text
    except Exception as e:
        logger.error(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ HTML —Å—Ç–∞—Ç—å–∏: {e}")
        return None

    meta_path = OUTPUT_DIR / f"{aid}_{slug}" / "meta.json"
    curr_hash = hashlib.sha256(html_txt.encode()).hexdigest()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è
    if meta_path.exists():
        try:
            m = json.loads(meta_path.read_text(encoding="utf-8"))
            if m.get("hash") == curr_hash:
                logger.info(f"   ‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫: –°—Ç–∞—Ç—å—è –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å (—Ö—ç—à —Å–æ–≤–ø–∞–¥–∞–µ—Ç).")
                return m
        except: pass

    soup = BeautifulSoup(html_txt, "html.parser")
    
    # --- –ò–©–ï–ú –í–ò–î–ï–û –î–û –û–ß–ò–°–¢–ö–ò ---
    video_url = None
    if iframe := soup.find("iframe"):
        src = iframe.get("src", "")
        if "youtube" in src or "youtu.be" in src:
            video_url = src

    # –û—á–∏—Å—Ç–∫–∞ –º—É—Å–æ—Ä–∞
    for r in soup.find_all("div", class_="post-widget-thumbnail"): r.decompose()
    for j in soup.find_all(["span", "div", "script", "style", "iframe"]):
        if not hasattr(j, 'attrs') or j.attrs is None: continue 
        c = str(j.get("class", ""))
        if j.get("data-mce-type") or "mce_SELRES" in c or "widget" in c: j.decompose()

    paras = []
    if c_div := soup.find("div", class_="entry-content"):
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—Å—Ç–∞
        for r in c_div.find_all(["ul", "ol", "div"], class_=re.compile(r"rp4wp|related|ad-")): r.decompose()
        paras = [sanitize_text(p.get_text(strip=True)) for p in c_div.find_all("p")]
    
    raw_body_text = BAD_RE.sub("", "\n\n".join(paras))
    
    # –°–±–æ—Ä –∫–∞—Ä—Ç–∏–Ω–æ–∫
    srcs = set()
    for link_tag in soup.find_all("a", class_="ci-lightbox", limit=10):
        if h := link_tag.get("href"): srcs.add(h)
    if c_div:
        for img in c_div.find_all("img"):
            if u := extract_img_url(img): srcs.add(u)
    
    images = []
    if srcs:
        logger.info(f"   üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ {len(srcs)} –∫–∞—Ä—Ç–∏–Ω–æ–∫. –°–∫–∞—á–∏–≤–∞–µ–º...")
        with ThreadPoolExecutor(5) as ex:
            futs = {ex.submit(save_image, u, OUTPUT_DIR / f"{aid}_{slug}" / "images"): u for u in list(srcs)[:10]}
            for f in as_completed(futs):
                if p:=f.result(): images.append(p)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ, –µ—Å–ª–∏ –µ—Å—Ç—å
    processed_video = None
    if video_url:
        processed_video = process_video_logic(video_url)

    # AI –∏ –ü–µ—Ä–µ–≤–æ–¥
    final_title, translated_body = title, raw_body_text
    if lang:
        final_title, translated_body = smart_process_and_translate(title, raw_body_text, lang)

    art_dir = OUTPUT_DIR / f"{aid}_{slug}"
    art_dir.mkdir(parents=True, exist_ok=True)
    
    meta = {
        "id": aid, 
        "slug": slug, 
        "title": final_title,
        "images": sorted([Path(p).name for p in images]), 
        "posted": False,
        "hash": curr_hash, 
        "video_url": video_url,
        "processed_video": processed_video
    }
    
    (art_dir / f"content.{lang}.txt").write_text(f"{final_title}\n\n{translated_body}", encoding="utf-8")
    with open(meta_path, "w", encoding="utf-8") as f: 
        json.dump(meta, f, ensure_ascii=False, indent=2)
    
    logger.info(f"   ‚úÖ –°—Ç–∞—Ç—å—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
    return meta

# --- MAIN ---
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--base-url", required=True)
    parser.add_argument("--slug", default="national")
    parser.add_argument("-n", "--limit", type=int, default=10)
    parser.add_argument("-l", "--lang", default="ru")
    parser.add_argument("--posted-state-file", default="articles/posted.json")
    parser.add_argument("--stopwords-file", default="stopwords.txt")
    args = parser.parse_args()

    try:
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        cleanup_old_articles(Path(args.posted_state_file), OUTPUT_DIR)
        
        cid = fetch_cat_id(args.base_url, args.slug)
        posts = fetch_posts(args.base_url, cid, FETCH_DEPTH)
        
        posted = load_posted_ids(Path(args.posted_state_file))
        stop = load_stopwords(Path(args.stopwords_file))
        
        catalog = []
        if CATALOG_PATH.exists():
            with open(CATALOG_PATH, 'r') as f: 
                try: catalog = json.load(f)
                except: catalog = []

        processed_count = 0
        new_items = []
        
        for post in posts:
            if processed_count >= args.limit:
                logger.info(f"üõë –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç {args.limit} —Å—Ç–∞—Ç–µ–π.")
                break
                
            if str(post["id"]) in posted:
                continue
                
            meta = parse_and_save(post, args.lang, stop)
            if meta:
                new_items.append(meta)
                processed_count += 1
        
        if new_items:
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞
            for m in new_items:
                catalog = [i for i in catalog if i.get("id") != m["id"]]
                catalog.append(m)
            
            with open(CATALOG_PATH, "w", encoding="utf-8") as f: 
                json.dump(catalog, f, indent=2, ensure_ascii=False)
            
            # –í–ê–ñ–ù–û: –¢–æ—Ç —Å–∞–º—ã–π –ø—Ä–∏–Ω—Ç –¥–ª—è GitHub Actions
            print("NEW_ARTICLES_STATUS:true")
            logger.info(f"üéâ –ó–∞–≤–µ—Ä—à–µ–Ω–æ! –î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π: {len(new_items)}")
        else:
            logger.info("‚ÑπÔ∏è –ù–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

    except Exception as e:
        logger.exception(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞:")
        exit(1)

if __name__ == "__main__":
    main()
