import random
import argparse
import logging
import json
import hashlib
import time
import re
import os
import shutil
import html
import fcntl
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Dict, List, Optional, Set

# –î–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º requests (—Å—Ç–∞–±–∏–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å Google GTX)
import requests 
from bs4 import BeautifulSoup
# –î–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º curl_cffi —Å –ø—Ä–æ—Ñ–∏–ª–µ–º Safari (—á—Ç–æ–±—ã —Å–∞–π—Ç –Ω–µ –±–∞–Ω–∏–ª)
from curl_cffi import requests as cffi_requests, CurlHttpVersion

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
OUTPUT_DIR = Path("articles")
CATALOG_PATH = OUTPUT_DIR / "catalog.json"
MAX_RETRIES = 3
BASE_DELAY = 1.0
MAX_POSTED_RECORDS = 300
FETCH_DEPTH = 100

# --- –ù–ê–°–¢–†–û–ô–ö–ò AI (OPENROUTER) ---
# –ö–ª—é—á —Ç–µ–ø–µ—Ä—å –æ–¥–∏–Ω, —Ç–∞–∫ –∫–∞–∫ OpenRouter –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä
OPENROUTER_KEY = os.getenv("OPENROUTER_API_KEY")

# –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –°–Ω–∞—á–∞–ª–∞ —É–º–Ω—ã–π –∏ –¥–µ—à–µ–≤—ã–π DeepSeek, –µ—Å–ª–∏ –æ–Ω –ª–µ–∂–∏—Ç ‚Äî —Å—Ç–∞–±–∏–ª—å–Ω—ã–π GPT-4o-mini
AI_MODELS = [
    "deepseek/deepseek-chat",           # Top-1: DeepSeek V3 (–£–º–Ω—ã–π, –¥–µ—à–µ–≤—ã–π)
    "openai/gpt-4o-mini",               # Top-2: GPT-4o-mini (–°—É–ø–µ—Ä-—Å—Ç–∞–±–∏–ª—å–Ω—ã–π –±—ç–∫–∞–ø)
    "google/gemini-2.0-flash-exp:free", # Top-3: –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π —Ä–µ–∑–µ—Ä–≤
]

# main.py

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞ –¥–ª—è –ø–æ—Ä—Ç–∞ WARP (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 40000)
WARP_PROXY = "socks5h://127.0.0.1:40000"

SCRAPER = cffi_requests.Session(
    impersonate="chrome110",
    # –¢–µ–ø–µ—Ä—å –≤–µ—Å—å —Ç—Ä–∞—Ñ–∏–∫ —Å–∫—Ä–∞–ø–µ—Ä–∞ –∏–¥–µ—Ç —á–µ—Ä–µ–∑ WARP
    proxies={
        "http": WARP_PROXY,
        "https": WARP_PROXY
    },
    http_version=CurlHttpVersion.V1_1
)

# –ù–µ –∑–∞–±—É–¥—å –æ–±–Ω–æ–≤–∏—Ç—å Plan B (–æ–±—ã—á–Ω—ã–µ requests)
# r = requests.get(endpoint, headers=FALLBACK_HEADERS, proxies={"https": WARP_PROXY}, timeout=30)

# –≠—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏–º–∏—Ç–∏—Ä—É—é—Ç –ø–µ—Ä–µ—Ö–æ–¥ –∏–∑ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞
IPHONE_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9",
    "Referer": "https://www.google.com/",
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "cross-site",
    "Sec-Fetch-User": "?1",
    "Upgrade-Insecure-Requests": "1"
}

# --- –ë–õ–û–ö 1: –ü–ï–†–ï–í–û–î –ò –ò–ò ---

def direct_google_translate(text: str, to_lang: str = "ru") -> str:
    """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ Google API (GTX) —Å —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ –∫—É—Å–∫–∏."""
    if not text: return ""
    
    chunks = []
    current_chunk = ""
    for paragraph in text.split('\n'):
        if len(current_chunk) + len(paragraph) < 1800:
            current_chunk += paragraph + "\n"
        else:
            chunks.append(current_chunk)
            current_chunk = paragraph + "\n"
    if current_chunk: chunks.append(current_chunk)
    
    translated_parts = []
    url = "https://translate.googleapis.com/translate_a/single"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"}
    
    for chunk in chunks:
        if not chunk.strip():
            translated_parts.append("")
            continue
        try:
            params = {"client": "gtx", "sl": "en", "tl": to_lang, "dt": "t", "q": chunk.strip()}
            r = requests.get(url, params=params, headers=headers, timeout=10)
            if r.status_code == 200:
                data = r.json()
                text_part = "".join([item[0] for item in data[0] if item and item[0]])
                translated_parts.append(text_part)
            else:
                translated_parts.append(chunk)
            time.sleep(0.3)
        except Exception:
            translated_parts.append(chunk)
            
    return "\n".join(translated_parts)

def strip_ai_chatter(text: str) -> str:
    bad_prefixes = ["Here is", "The article", "Summary:", "Cleaned text:"]
    for prefix in bad_prefixes:
        if text.lower().startswith(prefix.lower()):
            parts = text.split('\n', 1)
            if len(parts) > 1: return parts[1].strip()
    return text

def smart_process_and_translate(title: str, body: str, lang: str) -> (str, str):
    clean_body = body

    if OPENROUTER_KEY and len(body) > 500:
        logging.info("‚è≥ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –ò–ò-—á–∏—Å—Ç–∫–µ (OpenRouter)...")
        
        # 1. –ó–∞—â–∏—Ç–∞ –æ—Ç 400 Bad Request: —É–±–∏—Ä–∞–µ–º –Ω—É–ª–µ–≤—ã–µ –±–∞–π—Ç—ã
        safe_body = body[:15000].replace('\x00', '')
        
        prompt = (
            f"You are a ruthless news editor.\n"
            f"INPUT: Raw news text.\n"
            f"OUTPUT: A cleaned-up version of the story in ENGLISH.\n\n"
            "STRICT EDITING RULES:\n"
            "1. CONSOLIDATE NARRATIVE & SPEECH: If the author states a fact, and then a speaker repeats the same meaning, DELETE the speaker's part.\n"
            "2. KEEP UNIQUE DETAILS: Only keep quotes if they add numbers, dates, or emotion.\n"
            "3. REMOVE FLUFF: Delete ads and diplomatic praise.\n"
            "4. NO META-TALK: Start with the story immediately.\n\n"
            f"RAW TEXT:\n{safe_body}"
        )
        
        ai_result = ""
        
        # 2. –†–æ—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π (DeepSeek -> GPT-4o -> Gemini)
        for model in AI_MODELS:
            try:
                logging.info(f"üöÄ –ó–∞–ø—Ä–æ—Å –∫ OpenRouter: {model}...")
                
                # 3. FIX 400 ERROR: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä json= –≤–º–µ—Å—Ç–æ data=json.dumps
                response = requests.post(
                    url="https://openrouter.ai/api/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {OPENROUTER_KEY}",
                        "HTTP-Referer": "https://github.com/kh-news-bot",
                        "X-Title": "NewsBot",
                        # Content-Type –ø—Ä–æ—Å—Ç–∞–≤–∏—Ç—Å—è —Å–∞–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
                    },
                    json={
                        "model": model,
                        "messages": [{"role": "user", "content": prompt}],
                        "temperature": 0.3,
                        # –£ DeepSeek –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ–≥—Ä–æ–º–Ω—ã–π, –Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏–º —Ä–∞–∑—É–º–Ω–æ
                        "max_tokens": 4096 
                    },
                    timeout=50 # DeepSeek –∏–Ω–æ–≥–¥–∞ –¥—É–º–∞–µ—Ç –¥–æ–ª—å—à–µ –æ–±—ã—á–Ω–æ–≥–æ
                )
                
                if response.status_code == 200:
                    result = response.json()
                    if 'choices' in result and result['choices']:
                        ai_result = result['choices'][0]['message']['content'].strip()
                        logging.info(f"‚úÖ –£—Å–ø–µ—Ö! –ú–æ–¥–µ–ª—å: {model}")
                        break # –í—ã—Ö–æ–¥–∏–º, –≤—Å—ë –ø–æ–ª—É—á–∏–ª–æ—Å—å
                
                else:
                    # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ - –ª–æ–≥–∏—Ä—É–µ–º –∏ –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å
                    # 402 - –∫–æ–Ω—á–∏–ª–∏—Å—å –¥–µ–Ω—å–≥–∏, 503 - –ø–µ—Ä–µ–≥—Ä—É–∑
                    logging.warning(f"‚ö†Ô∏è –°–±–æ–π {model} (–ö–æ–¥ {response.status_code}): {response.text[:100]}")
                    continue

            except Exception as e:
                logging.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ —Å {model}: {e}")
                continue
        
        if ai_result:
            clean_body = strip_ai_chatter(ai_result)
        else:
            logging.warning("‚ùå –í—Å–µ –º–æ–¥–µ–ª–∏ –ò–ò –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –∏–ª–∏ –≤–µ—Ä–Ω—É–ª–∏ –æ—à–∏–±–∫—É. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç.")

    # –ö–û–ù–¢–ï–ö–°–¢–ù–´–ô –ü–ï–†–ï–í–û–î (Google) - –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
    DELIMITER = " ||| "
    combined_text = f"{title}{DELIMITER}{clean_body}"
    
    logging.info(f"üåç [Google] –ü–µ—Ä–µ–≤–æ–¥...")
    translated_full = direct_google_translate(combined_text, lang)
    
    final_title = title
    final_text = clean_body

    if translated_full:
        if DELIMITER in translated_full:
            parts = translated_full.split(DELIMITER, 1)
            final_title = parts[0].strip()
            final_text = parts[1].strip()
        elif "|||" in translated_full:
            parts = translated_full.split("|||", 1)
            final_title = parts[0].strip()
            final_text = parts[1].strip()
        else:
            parts = translated_full.split('\n', 1)
            final_title = parts[0].strip()
            final_text = parts[1].strip() if len(parts) > 1 else ""

    return final_title, final_text

# --- –ë–õ–û–ö 2: –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ---

def cleanup_old_articles(posted_ids_path: Path, articles_dir: Path):
    if not posted_ids_path.is_file() or not articles_dir.is_dir(): return
    try:
        with open(posted_ids_path, 'r', encoding='utf-8') as f:
            all_posted = json.load(f)
            ids_to_keep = set(str(x) for x in all_posted[-MAX_POSTED_RECORDS:])
        cleaned = 0
        for f in articles_dir.iterdir():
            if f.is_dir():
                parts = f.name.split('_', 1)
                if parts and parts[0].isdigit():
                    if parts[0] not in ids_to_keep:
                        shutil.rmtree(f); cleaned += 1
        if cleaned: logging.info(f"üßπ –£–¥–∞–ª–µ–Ω–æ {cleaned} —Å—Ç–∞—Ä—ã—Ö –ø–∞–ø–æ–∫.")
    except Exception: pass

def sanitize_text(text: str) -> str:
    if not text: return ""
    text = html.unescape(text)
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'mce_SELRES_[^ ]+', '', text)
    return re.sub(r'\n{3,}', '\n\n', text).strip()

def load_posted_ids(state_file_path: Path) -> Set[str]:
    try:
        if state_file_path.exists():
            with open(state_file_path, 'r', encoding='utf-8') as f:
                fcntl.flock(f, fcntl.LOCK_SH)
                return {str(item) for item in json.load(f)}
        return set()
    except Exception: return set()

def load_stopwords(file_path: Optional[Path]) -> List[str]:
    if not file_path or not file_path.exists(): return []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return [line.strip().lower() for line in f if line.strip()]
    except Exception: return []

# --- –ë–õ–û–ö 3: –£–ú–ù–´–ô –ü–û–ò–°–ö –ö–ê–†–¢–ò–ù–û–ö ---

def extract_img_url(img_tag: Any) -> Optional[str]:
    def is_junk(url_str: str) -> bool:
        u = url_str.lower()
        bad = ["gif", "logo", "banner", "icon", "avatar", "button", "share", "pixel", "tracker"]
        if any(b in u for b in bad): return True
        if re.search(r'-\d{2,3}x\d{2,3}\.', u): return True
        return False

    # 1. –°–¢–†–ê–¢–ï–ì–ò–Ø ‚Ññ1: –ò—â–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π —Å—Å—ã–ª–∫–µ (Lightbox)
    parent_a = img_tag.find_parent("a")
    if parent_a:
        href = parent_a.get("href")
        if href and any(href.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.webp']):
            if not is_junk(href):
                return href.split('?')[0]

    # 2. –°–¢–†–ê–¢–ï–ì–ò–Ø ‚Ññ2: –ï—Å–ª–∏ —Å—Å—ã–ª–∫–∏ –Ω–µ—Ç, –∫–æ–ø–∞–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã Breeze (data-brsrcset)
    srcset = img_tag.get("data-brsrcset") or img_tag.get("srcset") or img_tag.get("data-srcset")
    if srcset:
        try:
            links = []
            for p in srcset.split(','):
                match = re.search(r'(\S+)\s+(\d+)w', p.strip())
                if match:
                    w_val = int(match.group(2))
                    u_val = match.group(1)
                    if w_val >= 400:
                        links.append((w_val, u_val))
            if links:
                best_link = sorted(links, key=lambda x: x[0], reverse=True)[0][1]
                if not is_junk(best_link):
                    return best_link.split('?')[0]
        except Exception: pass

    # 3. –°–¢–†–ê–¢–ï–ì–ò–Ø ‚Ññ3: –ü—Ä–æ–≤–µ—Ä–∫–∞ —à–∏—Ä–∏–Ω—ã –∏ –ø—Ä—è–º—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤
    width_attr = img_tag.get("width")
    if width_attr and width_attr.isdigit() and int(width_attr) < 300:
        return None

    for attr in ["data-breeze", "data-src", "src"]:
        val = img_tag.get(attr)
        if val:
            clean_url = val.split()[0].split(',')[0].split('?')[0]
            if not is_junk(clean_url):
                return clean_url

    return None

# --- –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ë–õ–û–ö –°–û–•–†–ê–ù–ï–ù–ò–Ø –ö–ê–†–¢–ò–ù–û–ö ---
def save_image(url, folder):
    if not url or url.startswith('data:'): return None
    
    folder.mkdir(parents=True, exist_ok=True)
    
    url_hash = hashlib.md5(url.encode()).hexdigest()[:10]
    orig_fn = url.rsplit('/', 1)[-1].split('?', 1)[0]
    ext = orig_fn.split('.')[-1] if '.' in orig_fn else 'jpg'
    if len(ext) > 4: ext = 'jpg'
    
    fn = f"{url_hash}.{ext}"
    dest = folder / fn
    
    # 1. –ü—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ SCRAPER (Chrome/Safari)
    try:
        resp = SCRAPER.get(url, timeout=20)
        if resp.status_code == 200:
            dest.write_bytes(resp.content)
            return str(dest)
    except Exception:
        pass # –ï—Å–ª–∏ –Ω–µ –≤—ã—à–ª–æ, –∏–¥–µ–º –∫ –ü–ª–∞–Ω—É –ë

    # 2. –ü–ª–∞–Ω –ë: –û–±—ã—á–Ω—ã–π requests (–¥–ª—è –∫–∞—Ä—Ç–∏–Ω–æ–∫ —á–∞—Å—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ)
    try:
        resp = requests.get(url, headers=FALLBACK_HEADERS, timeout=20)
        if resp.status_code == 200:
            dest.write_bytes(resp.content)
            return str(dest)
    except Exception as e:
        logging.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å —Ñ–æ—Ç–æ {url}: {e}")
    
    return None

# --- –ë–õ–û–ö 4: API –ò –ü–ê–†–°–ò–ù–ì ---

def fetch_cat_id(url, slug):
    endpoint = f"{url}/wp-json/wp/v2/categories?slug={slug}"
    
    # –ü—ã—Ç–∞–µ–º—Å—è 3 —Ä–∞–∑–∞ —Å –ø–∞—É–∑–æ–π –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
    for attempt in range(1, 4):
        try:
            logging.info(f"üì° –ü–æ–ø—ã—Ç–∫–∞ {attempt}: –ó–∞–ø—Ä–æ—Å –∫ API...")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º curl_cffi (Plan A)
            r = SCRAPER.get(endpoint, timeout=30)
            
            # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ HTML –≤–º–µ—Å—Ç–æ JSON, –∑–Ω–∞—á–∏—Ç —ç—Ç–æ –∑–∞–≥–ª—É—à–∫–∞ Cloudflare
            if "text/html" in r.headers.get("Content-Type", ""):
                logging.warning(f"‚ö†Ô∏è –ü–æ–ª—É—á–µ–Ω HTML –≤–º–µ—Å—Ç–æ JSON (Cloudflare Challenge).")
            else:
                r.raise_for_status()
                return r.json()[0]["id"]
                
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
        
        # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å ‚Äî –∂–¥–µ–º –ø–æ–¥–æ–ª—å—à–µ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π
        wait_time = attempt * 10 
        logging.info(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {wait_time} —Å–µ–∫ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º...")
        time.sleep(wait_time)

    # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–æ–≤–∞–ª–µ–Ω—ã ‚Äî –ø—Ä–æ–±—É–µ–º Plan B –Ω–∞–ø–æ—Å–ª–µ–¥–æ–∫
    logging.error("üö® –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ —á–µ—Ä–µ–∑ WARP/Chrome –ø—Ä–æ–≤–∞–ª–µ–Ω—ã. –ü—Ä–æ–±—É–µ–º Plan B...")
    try:
        r = requests.get(endpoint, headers=IPHONE_HEADERS, proxies={"https": WARP_PROXY}, timeout=30)
        r.raise_for_status()
        return r.json()[0]["id"]
    except Exception as e:
        logging.error(f"üíÄ –§–∏–Ω–∞–ª—å–Ω—ã–π –∫—Ä–∞—Ö: {e}")
        raise

def fetch_posts_light(url: str, cid: int, limit: int) -> List[Dict]:
    params = {"categories": cid, "per_page": limit, "_fields": "id,slug"}
    endpoint = f"{url}/wp-json/wp/v2/posts"
    try:
        r = SCRAPER.get(endpoint, params=params, timeout=30)
        r.raise_for_status()
        return r.json()
    except Exception:
        logging.warning("‚ö†Ô∏è –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ Plan B –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π...")
        r = requests.get(endpoint, params=params, headers=FALLBACK_HEADERS, timeout=30)
        return r.json()

def fetch_single_post_full(url: str, aid: str) -> Optional[Dict]:
    """–¢–Ø–ñ–ï–õ–´–ô –∑–∞–ø—Ä–æ—Å: –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏ —Å–æ –≤—Å–µ–º–∏ –≤–ª–æ–∂–µ–Ω–∏—è–º–∏."""
    try:
        # –ó–¥–µ—Å—å –∏—Å–ø–æ–ª—å–∑—É–µ–º _embed, —Ç–∞–∫ –∫–∞–∫ —Ç—è–Ω–µ–º —Ç–æ–ª—å–∫–æ –û–î–ù–£ —Å—Ç–∞—Ç—å—é
        r = SCRAPER.get(f"{url}/wp-json/wp/v2/posts/{aid}?_embed", timeout=60)
        r.raise_for_status()
        return r.json()
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è ID={aid}: {e}")
        return None

def parse_and_save(post, lang, stopwords):
    time.sleep(2)
    aid, slug, link = str(post["id"]), post["slug"], post.get("link")
    
    raw_title = BeautifulSoup(post["title"]["rendered"], "html.parser").get_text(strip=True)
    title = sanitize_text(raw_title)

    if stopwords:
        for ph in stopwords:
            if ph in title.lower():
                logging.info(f"üö´ ID={aid}: –°—Ç–æ–ø-—Å–ª–æ–≤–æ '{ph}'")
                return None

    # === –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ó–î–ï–°–¨ (PLAN B –¥–ª—è —Ç–µ–ª–∞ —Å—Ç–∞—Ç—å–∏) ===
    html_txt = ""
    # 1. –ü–ª–∞–Ω –ê: Scraper
    try:
        resp = SCRAPER.get(link, timeout=30)
        if resp.status_code == 200:
            html_txt = resp.text
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è ID={aid}: Scraper –Ω–µ –æ—Ç–∫—Ä—ã–ª —Å—Å—ã–ª–∫—É ({e}). –ü—Ä–æ–±—É–µ–º requests...")

    # 2. –ü–ª–∞–Ω –ë: Requests
    if not html_txt:
        try:
            resp = requests.get(link, headers=FALLBACK_HEADERS, timeout=30)
            if resp.status_code == 200:
                html_txt = resp.text
            else:
                logging.error(f"‚ùå ID={aid}: –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ HTML {resp.status_code}")
                return None
        except Exception as e:
            logging.error(f"‚ùå ID={aid}: –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å —Å—Ç–∞—Ç—å—é: {e}")
            return None
    # =========================

    meta_path = OUTPUT_DIR / f"{aid}_{slug}" / "meta.json"
    curr_hash = hashlib.sha256(html_txt.encode()).hexdigest()
    if meta_path.exists():
        try:
            m = json.loads(meta_path.read_text(encoding="utf-8"))
            if m.get("hash") == curr_hash:
                logging.info(f"‚è≠Ô∏è ID={aid}: –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π.")
                return m
        except: pass

    logging.info(f"Processing ID={aid}: {title[:30]}...")

    soup = BeautifulSoup(html_txt, "html.parser")
    
    # –ì–ª–æ–±–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –º—É—Å–æ—Ä–∞
    for garbage in soup.find_all(["div", "ul", "ol", "section", "aside"], 
                                class_=re.compile(r"rp4wp|related|ad-|post-widget-thumbnail|sharedaddy")):
        garbage.decompose()

    for j in soup.find_all(["span", "script", "style", "iframe"]):
        if not hasattr(j, 'attrs') or j.attrs is None: continue 
        c = str(j.get("class", ""))
        if j.get("data-mce-type") or "mce_SELRES" in c or "widget" in c: 
            j.decompose()

    # –°–±–æ—Ä URL
    ordered_srcs = []
    seen_srcs = set()

    def add_src(url):
        if url and url not in seen_srcs:
            ordered_srcs.append(url)
            seen_srcs.add(url)

    if "_embedded" in post and (m := post["_embedded"].get("wp:featuredmedia")):
        if isinstance(m, list) and (u := m[0].get("source_url")):
            if "logo" not in u.lower():
                add_src(u)

    for link_tag in soup.find_all("a", class_="ci-lightbox", limit=10):
        if h := link_tag.get("href"): 
            if "gif" not in h.lower():
                add_src(h)

    c_div = soup.find("div", class_="entry-content")
    if c_div:
        for img in c_div.find_all("img"):
            if u := extract_img_url(img):
                add_src(u)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫
    images_results = [None] * len(ordered_srcs)
    if ordered_srcs:
        # –£–º–µ–Ω—å—à–∞–µ–º –∫–æ–ª-–≤–æ –ø–æ—Ç–æ–∫–æ–≤, —á—Ç–æ–±—ã –Ω–µ –∑–∞–±–∞–Ω–∏–ª–∏ –∑–∞ DDOS
        with ThreadPoolExecutor(3) as ex:
            future_to_idx = {
                ex.submit(save_image, url, OUTPUT_DIR / f"{aid}_{slug}" / "images"): i 
                for i, url in enumerate(ordered_srcs[:10])
            }
            for f in as_completed(future_to_idx):
                idx = future_to_idx[f]
                if res := f.result():
                    images_results[idx] = Path(res).name

    final_images = [img for img in images_results if img is not None]

    if not final_images:
        logging.warning(f"‚ö†Ô∏è ID={aid}: –ù–µ—Ç –Ω–æ—Ä–º –∫–∞—Ä—Ç–∏–Ω–æ–∫. Skip.")
        return None

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
    paras = []
    if c_div:
        for r in c_div.find_all(["ul", "ol", "div"], class_=re.compile(r"rp4wp|related|ad-")): 
            r.decompose()
        paras = [sanitize_text(p.get_text(strip=True)) for p in c_div.find_all("p")]
    
    raw_body_text = "\n\n".join(paras)

    # –û–ë–†–ê–ë–û–¢–ö–ê + –ü–ï–†–ï–í–û–î
    final_title = title
    translated_body = ""
    if lang:
        final_title, translated_body = smart_process_and_translate(title, raw_body_text, lang)
        final_title = sanitize_text(final_title)

    art_dir = OUTPUT_DIR / f"{aid}_{slug}"
    art_dir.mkdir(parents=True, exist_ok=True)
    
    (art_dir / "content.txt").write_text(raw_body_text, encoding="utf-8")
    
    meta = {
        "id": aid, "slug": slug, "date": post.get("date"), "link": link,
        "title": final_title, "text_file": "content.txt",
        "images": final_images, "posted": False,
        "hash": curr_hash, "translated_to": ""
    }

    if translated_body:
        (art_dir / f"content.{lang}.txt").write_text(f"{final_title}\n\n{translated_body}", encoding="utf-8")
        meta.update({"translated_to": lang, "text_file": f"content.{lang}.txt"})

    with open(meta_path, "w", encoding="utf-8") as f: 
        json.dump(meta, f, ensure_ascii=False, indent=2)
    return meta

# --- MAIN ---
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--base-url", required=True)
    parser.add_argument("--slug", default="national")
    parser.add_argument("-n", "--limit", type=int, default=10)
    parser.add_argument("-l", "--lang", default="ru")
    parser.add_argument("--posted-state-file", default="articles/posted.json")
    parser.add_argument("--stopwords-file", default="stopwords.txt")
    args = parser.parse_args()

    try:
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –ø–∞–ø–∫–∏ —Å—Ç–∞—Ç–µ–π, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 100 –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö
        cleanup_old_articles(Path(args.posted_state_file), OUTPUT_DIR)
        
        # –ü–æ–ª—É—á–∞–µ–º ID –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        cid = fetch_cat_id(args.base_url, args.slug)
        
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ: –ª–µ–≥–∫–∏–π —Å–ø–∏—Å–æ–∫ ID, –∏—Å—Ç–æ—Ä–∏—é –∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        posts_light = fetch_posts_light(args.base_url, cid, FETCH_DEPTH)
        posted = load_posted_ids(Path(args.posted_state_file))
        stop = load_stopwords(Path(args.stopwords_file))
        
        # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â–∏–π –∫–∞—Ç–∞–ª–æ–≥ –∏–∑ —Ñ–∞–π–ª–∞
        catalog = []
        if CATALOG_PATH.exists():
            try:
                with open(CATALOG_PATH, 'r', encoding='utf-8') as f:
                    catalog = json.load(f)
            except Exception:
                logging.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–∞—Ç–∞–ª–æ–≥. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π.")

        new_metas = []
        count = 0
        
        # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏
        for p_short in posts_light:
            if count >= args.limit:
                break
            
            aid = str(p_short["id"])
            if aid in posted:
                continue # –≠—Ç—É —Å—Ç–∞—Ç—å—é —É–∂–µ –ø–æ—Å—Ç–∏–ª–∏, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            
            logging.info(f"üÜï –ù–∞–π–¥–µ–Ω–∞ –Ω–æ–≤–∞—è —Å—Ç–∞—Ç—å—è ID={aid}. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–µ—Ç–∞–ª–∏...")
            full_post = fetch_single_post_full(args.base_url, aid)
            
            if full_post:
                # parse_and_save –≤–Ω—É—Ç—Ä–∏ —Å–µ–±—è –¥–µ–ª–∞–µ—Ç AI-—á–∏—Å—Ç–∫—É –∏ –ø–µ—Ä–µ–≤–æ–¥
                if meta := parse_and_save(full_post, args.lang, stop):
                    new_metas.append(meta)
                    count += 1

        # 3. –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞ –∏ –æ—Ç—á–µ—Ç –¥–ª—è GitHub Actions
        if new_metas:
            # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º –¥—É–±–ª–∏ –≤ –∫–∞—Ç–∞–ª–æ–≥–µ: —É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ —Å —Ç–µ–º–∏ –∂–µ ID
            new_ids = {str(m["id"]) for m in new_metas}
            catalog = [item for item in catalog if str(item.get("id")) not in new_ids]
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤–µ–∂–µ–ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            catalog.extend(new_metas)
            
            with open(CATALOG_PATH, "w", encoding="utf-8") as f: 
                json.dump(catalog, f, ensure_ascii=False, indent=2)
            
            # –≠—Ç–æ —Å–∏–≥–Ω–∞–ª –¥–ª—è GitHub Actions, —á—Ç–æ –Ω—É–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –ø–æ—Å—Ç–µ—Ä
            print("NEW_ARTICLES_STATUS:true")
            logging.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –î–æ–±–∞–≤–ª–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(new_metas)}")
        else:
            print("NEW_ARTICLES_STATUS:false")
            logging.info("üîç –ù–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

    except Exception:
        logging.exception("üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ main:")
        exit(1)

if __name__ == "__main__":
    main()
