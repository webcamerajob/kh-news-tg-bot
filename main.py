import random
import argparse
import logging
import json
import hashlib
import time
import re
import os
import shutil
import html
import fcntl
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Dict, List, Optional, Set

# –î–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º requests (—Å—Ç–∞–±–∏–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å Google GTX)
import requests 
from bs4 import BeautifulSoup
# –î–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º curl_cffi —Å –ø—Ä–æ—Ñ–∏–ª–µ–º Safari (—á—Ç–æ–±—ã —Å–∞–π—Ç –Ω–µ –±–∞–Ω–∏–ª)
from curl_cffi import requests as cffi_requests, CurlHttpVersion

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
OUTPUT_DIR = Path("articles")
CATALOG_PATH = OUTPUT_DIR / "catalog.json"
MAX_RETRIES = 3
BASE_DELAY = 1.0
MAX_POSTED_RECORDS = 300
FETCH_DEPTH = 100

GROQ_API_KEY = os.getenv("GROQ_API_KEY")

# –ß–∏—Ç–∞–µ–º –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É —Å –∫–ª—é—á–∞–º–∏ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é
raw_keys = os.getenv("GROQ_KEYS", "")
# –†–∞–∑–±–∏–≤–∞–µ–º, —É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –ø—É—Å—Ç—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
GROQ_KEYS = [k.strip() for k in raw_keys.split(",") if k.strip()]

if GROQ_KEYS:
    logging.info(f"üîë –ü—É–ª –∫–ª—é—á–µ–π Groq –≥–æ—Ç–æ–≤. –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∫–ª—é—á–µ–π: {len(GROQ_KEYS)}")
else:
    logging.warning("‚ö†Ô∏è –ö–ª—é—á–∏ Groq –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π GROQ_KEYS!")
    
AI_MODELS = [
    "llama-3.3-70b-versatile",  # –¢–æ–ø–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ—Ç–ª–∏—á–Ω–æ –ø–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç
    "llama-3.1-70b-versatile",  # –ü—Ä–µ–¥—ã–¥—É—â–∞—è –≤–µ—Ä—Å–∏—è, —Ç–æ–∂–µ —Ö–æ—Ä–æ—à–∞
    "mixtral-8x7b-32768",       # –•–æ—Ä–æ—à–∏–π –±—ç–∫–∞–ø
    "llama-3.1-8b-instant",     # –û—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è, –µ—Å–ª–∏ –ª–∏–º–∏—Ç—ã –Ω–∞ 70b –∫–æ–Ω—á–∏–ª–∏—Å—å
]

# --- –ù–ê–°–¢–†–û–ô–ö–ò –°–ï–¢–ò (PARSER) ---
# –ò–°–ü–†–ê–í–õ–ï–ù–û: –í–µ—Ä–Ω—É–ª Safari –∏ —É–±—Ä–∞–ª –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π HTTP/1.1
# –≠—Ç–æ —Ä–µ—à–∏—Ç –ø—Ä–æ–±–ª–µ–º—É —Å Timeout –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ.
SCRAPER = cffi_requests.Session(
    impersonate="chrome110",
    http_version=CurlHttpVersion.V1_1
)

SCRAPER.headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.5 Safari/605.1.15",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9",
    "Referer": "https://www.google.com/"
}
# –£–≤–µ–ª–∏—á–∏–ª —Ç–∞–π–º–∞—É—Ç –¥–æ 60 —Å–µ–∫ –¥–ª—è –º–µ–¥–ª–µ–Ω–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏/VPN
SCRAPER_TIMEOUT = 60 
BAD_RE = re.compile(r"[\u200b-\u200f\uFEFF\u200E\u00A0]")

# --- –ë–õ–û–ö 1: –ü–ï–†–ï–í–û–î –ò –ò–ò ---

def direct_google_translate(text: str, to_lang: str = "ru") -> str:
    """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ Google API (GTX) —Å —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ –∫—É—Å–∫–∏."""
    if not text: return ""
    
    chunks = []
    current_chunk = ""
    for paragraph in text.split('\n'):
        if len(current_chunk) + len(paragraph) < 1800:
            current_chunk += paragraph + "\n"
        else:
            chunks.append(current_chunk)
            current_chunk = paragraph + "\n"
    if current_chunk: chunks.append(current_chunk)
    
    translated_parts = []
    url = "https://translate.googleapis.com/translate_a/single"
    # –û–±—ã—á–Ω—ã–π User-Agent –¥–ª—è requests (Google –µ–≥–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç)
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"}
    
    for chunk in chunks:
        if not chunk.strip():
            translated_parts.append("")
            continue
        try:
            params = {"client": "gtx", "sl": "en", "tl": to_lang, "dt": "t", "q": chunk.strip()}
            # –¢–∞–π–º–∞—É—Ç 10 —Å–µ–∫ –Ω–∞ –∫—É—Å–æ–∫ –ø–µ—Ä–µ–≤–æ–¥–∞
            r = requests.get(url, params=params, headers=headers, timeout=10)
            if r.status_code == 200:
                data = r.json()
                text_part = "".join([item[0] for item in data[0] if item and item[0]])
                translated_parts.append(text_part)
            else:
                translated_parts.append(chunk)
            time.sleep(0.3)
        except Exception:
            translated_parts.append(chunk)
            
    return "\n".join(translated_parts)

def strip_ai_chatter(text: str) -> str:
    bad_prefixes = ["Here is", "The article", "Summary:", "Cleaned text:"]
    for prefix in bad_prefixes:
        if text.lower().startswith(prefix.lower()):
            parts = text.split('\n', 1)
            if len(parts) > 1: return parts[1].strip()
    return text

def smart_process_and_translate(title: str, body: str, lang: str) -> (str, str):
    clean_body = body

    if GROQ_KEYS and len(body) > 500:
        logging.info("‚è≥ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –ò–ò-—á–∏—Å—Ç–∫–µ...")
        
        prompt = (
            f"You are a ruthless news editor.\n"
            f"INPUT: Raw news text.\n"
            f"OUTPUT: A cleaned-up version of the story in ENGLISH.\n\n"
            "STRICT EDITING RULES:\n"
            "1. CONSOLIDATE NARRATIVE & SPEECH: If the author states a fact, and then a speaker repeats the same meaning, DELETE the speaker's part.\n"
            "2. KEEP UNIQUE DETAILS: Only keep quotes if they add numbers, dates, or emotion.\n"
            "3. REMOVE FLUFF: Delete ads and diplomatic praise.\n"
            "4. NO META-TALK: Start with the story immediately.\n\n"
            f"RAW TEXT:\n{body[:15000]}" # Groq –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –±–æ–ª—å—à–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        )
        
        ai_result = ""
        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –∫–ª—é—á–∏ –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏
        current_pool = list(GROQ_KEYS)
        random.shuffle(current_pool)

        # –ü–µ—Ä–µ–±–æ—Ä –∫–ª—é—á–µ–π
        for api_key in current_pool:
            if ai_result: break 

            logging.info(f"üöÄ –ü—Ä–æ–±—É–µ–º –∫–ª—é—á {api_key[:6]}...")
            
            # –ü–µ—Ä–µ–±–æ—Ä –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –∫–ª—é—á–∞
            for model in AI_MODELS:
                try:
                    response = requests.post(
                        url="https://api.groq.com/openai/v1/chat/completions",
                        headers={
                            "Authorization": f"Bearer {api_key}",
                            "Content-Type": "application/json"
                        },
                        data=json.dumps({
                            "model": model,
                            "messages": [{"role": "user", "content": prompt}],
                            "temperature": 0.3,
                            "max_tokens": 4096
                        }),
                        timeout=30
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        ai_result = result['choices'][0]['message']['content'].strip()
                        logging.info(f"‚úÖ –£—Å–ø–µ—Ö! –ú–æ–¥–µ–ª—å: {model} (–ö–ª—é—á: {api_key[:6]}...)")
                        break # –í—ã—Ö–æ–¥ –∏–∑ —Ü–∏–∫–ª–∞ –º–æ–¥–µ–ª–µ–π
                    
                    elif response.status_code == 429:
                        logging.warning(f"üê¢ Rate Limit –Ω–∞ –∫–ª—é—á–µ {api_key[:6]}... –ü—Ä–æ–±—É–µ–º –°–õ–ï–î–£–Æ–©–ò–ô –ö–õ–Æ–ß.")
                        break # –ü—Ä–µ—Ä—ã–≤–∞–µ–º —Ü–∏–∫–ª –º–æ–¥–µ–ª–µ–π, —á—Ç–æ–±—ã —Å–º–µ–Ω–∏—Ç—å –∫–ª—é—á
                    
                    else:
                        logging.error(f"‚ùå –û—à–∏–±–∫–∞ {response.status_code} –Ω–∞ –∫–ª—é—á–µ {api_key[:6]}...")
                        break # –ü—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π –∫–ª—é—á

                except Exception as e:
                    logging.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è (–ö–ª—é—á: {api_key[:6]}...): {e}")
                    break # –ü—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π –∫–ª—é—á
        
        if ai_result:
            clean_body = strip_ai_chatter(ai_result)

    # –ö–û–ù–¢–ï–ö–°–¢–ù–´–ô –ü–ï–†–ï–í–û–î (Google) - –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
    DELIMITER = " ||| "
    combined_text = f"{title}{DELIMITER}{clean_body}"
    
    logging.info(f"üåç [Google] –ü–µ—Ä–µ–≤–æ–¥...")
    translated_full = direct_google_translate(combined_text, lang)
    
    final_title = title
    final_text = clean_body

    if translated_full:
        if DELIMITER in translated_full:
            parts = translated_full.split(DELIMITER, 1)
            final_title = parts[0].strip()
            final_text = parts[1].strip()
        elif "|||" in translated_full:
            parts = translated_full.split("|||", 1)
            final_title = parts[0].strip()
            final_text = parts[1].strip()
        else:
            parts = translated_full.split('\n', 1)
            final_title = parts[0].strip()
            final_text = parts[1].strip() if len(parts) > 1 else ""

    return final_title, final_text

# --- –ë–õ–û–ö 2: –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ---

def cleanup_old_articles(posted_ids_path: Path, articles_dir: Path):
    if not posted_ids_path.is_file() or not articles_dir.is_dir(): return
    try:
        with open(posted_ids_path, 'r', encoding='utf-8') as f:
            all_posted = json.load(f)
            ids_to_keep = set(str(x) for x in all_posted[-MAX_POSTED_RECORDS:])
        cleaned = 0
        for f in articles_dir.iterdir():
            if f.is_dir():
                parts = f.name.split('_', 1)
                if parts and parts[0].isdigit():
                    if parts[0] not in ids_to_keep:
                        shutil.rmtree(f); cleaned += 1
        if cleaned: logging.info(f"üßπ –£–¥–∞–ª–µ–Ω–æ {cleaned} —Å—Ç–∞—Ä—ã—Ö –ø–∞–ø–æ–∫.")
    except Exception: pass

def sanitize_text(text: str) -> str:
    if not text: return ""
    text = html.unescape(text)
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'mce_SELRES_[^ ]+', '', text)
    return re.sub(r'\n{3,}', '\n\n', text).strip()

def load_posted_ids(state_file_path: Path) -> Set[str]:
    try:
        if state_file_path.exists():
            with open(state_file_path, 'r', encoding='utf-8') as f:
                fcntl.flock(f, fcntl.LOCK_SH)
                return {str(item) for item in json.load(f)}
        return set()
    except Exception: return set()

def load_stopwords(file_path: Optional[Path]) -> List[str]:
    if not file_path or not file_path.exists(): return []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return [line.strip().lower() for line in f if line.strip()]
    except Exception: return []

# --- –ë–õ–û–ö 3: –£–ú–ù–´–ô –ü–û–ò–°–ö –ö–ê–†–¢–ò–ù–û–ö ---

def extract_img_url(img_tag: Any) -> Optional[str]:
    def is_junk(url_str: str) -> bool:
        u = url_str.lower()
        bad = ["gif", "logo", "banner", "icon", "avatar", "button", "share", "pixel", "tracker"]
        if any(b in u for b in bad): return True
        if re.search(r'-\d{2,3}x\d{2,3}\.', u): return True
        return False

    # 1. –°–¢–†–ê–¢–ï–ì–ò–Ø ‚Ññ1: –ò—â–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π —Å—Å—ã–ª–∫–µ (Lightbox)
    # –í —Ç–≤–æ–µ–º –ø—Ä–∏–º–µ—Ä–µ —ç—Ç–æ <a href="...">...</a>
    parent_a = img_tag.find_parent("a")
    if parent_a:
        href = parent_a.get("href")
        if href and any(href.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.webp']):
            if not is_junk(href):
                return href.split('?')[0]

    # 2. –°–¢–†–ê–¢–ï–ì–ò–Ø ‚Ññ2: –ï—Å–ª–∏ —Å—Å—ã–ª–∫–∏ –Ω–µ—Ç, –∫–æ–ø–∞–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã Breeze (data-brsrcset)
    srcset = img_tag.get("data-brsrcset") or img_tag.get("srcset") or img_tag.get("data-srcset")
    if srcset:
        try:
            links = []
            for p in srcset.split(','):
                match = re.search(r'(\S+)\s+(\d+)w', p.strip())
                if match:
                    w_val = int(match.group(2))
                    u_val = match.group(1)
                    if w_val >= 400:
                        links.append((w_val, u_val))
            if links:
                best_link = sorted(links, key=lambda x: x[0], reverse=True)[0][1]
                if not is_junk(best_link):
                    return best_link.split('?')[0]
        except Exception: pass

    # 3. –°–¢–†–ê–¢–ï–ì–ò–Ø ‚Ññ3: –ü—Ä–æ–≤–µ—Ä–∫–∞ —à–∏—Ä–∏–Ω—ã –∏ –ø—Ä—è–º—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤
    width_attr = img_tag.get("width")
    if width_attr and width_attr.isdigit() and int(width_attr) < 300:
        return None

    for attr in ["data-breeze", "data-src", "src"]:
        val = img_tag.get(attr)
        if val:
            clean_url = val.split()[0].split(',')[0].split('?')[0]
            if not is_junk(clean_url):
                return clean_url

    return None

def save_image(url, folder):
    if not url or url.startswith('data:'): return None # –ò–≥–Ω–æ—Ä–∏–º base64 –º—É—Å–æ—Ä
    
    folder.mkdir(parents=True, exist_ok=True)
    
    url_hash = hashlib.md5(url.encode()).hexdigest()[:10]
    orig_fn = url.rsplit('/', 1)[-1].split('?', 1)[0]
    ext = orig_fn.split('.')[-1] if '.' in orig_fn else 'jpg'
    if len(ext) > 4: ext = 'jpg' # –ù–∞ —Å–ª—É—á–∞–π –∫—Ä–∏–≤—ã—Ö —Å—Å—ã–ª–æ–∫
    
    fn = f"{url_hash}.{ext}"
    dest = folder / fn
    
    try:
        # –ö–∞—á–∞–µ–º —á–µ—Ä–µ–∑ SCRAPER (Safari –ø—Ä–æ—Ñ–∏–ª—å)
        resp = SCRAPER.get(url, timeout=SCRAPER_TIMEOUT)
        if resp.status_code == 200:
            dest.write_bytes(resp.content)
            return str(dest)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–æ—Ç–æ {url}: {e}")
    return None

# --- –ë–õ–û–ö 4: API –ò –ü–ê–†–°–ò–ù–ì ---

def fetch_cat_id(url, slug):
    r = SCRAPER.get(f"{url}/wp-json/wp/v2/categories?slug={slug}", timeout=SCRAPER_TIMEOUT)
    r.raise_for_status(); data=r.json()
    if not data: raise RuntimeError("Cat not found")
    return data[0]["id"]

def fetch_posts_light(url: str, cid: int, limit: int) -> List[Dict]:
    """–õ–ï–ì–ö–ò–ô –∑–∞–ø—Ä–æ—Å: —Ç–æ–ª—å–∫–æ ID –∏ slug. WordPress –æ—Ç–¥–∞–µ—Ç —ç—Ç–æ –º–≥–Ω–æ–≤–µ–Ω–Ω–æ."""
    logging.info(f"üì° –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–ø–∏—Å–∫–∞ –∏–∑ {limit} –ø–æ—Å–ª–µ–¥–Ω–∏—Ö ID...")
    try:
        params = {
            "categories": cid, 
            "per_page": limit, 
            "_fields": "id,slug" # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–≤–∞ –ø–æ–ª—è
        }
        r = SCRAPER.get(f"{url}/wp-json/wp/v2/posts", params=params, timeout=30)
        r.raise_for_status()
        return r.json()
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ª–µ–≥–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {e}")
        return []

def fetch_single_post_full(url: str, aid: str) -> Optional[Dict]:
    """–¢–Ø–ñ–ï–õ–´–ô –∑–∞–ø—Ä–æ—Å: –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏ —Å–æ –≤—Å–µ–º–∏ –≤–ª–æ–∂–µ–Ω–∏—è–º–∏."""
    try:
        # –ó–¥–µ—Å—å –∏—Å–ø–æ–ª—å–∑—É–µ–º _embed, —Ç–∞–∫ –∫–∞–∫ —Ç—è–Ω–µ–º —Ç–æ–ª—å–∫–æ –û–î–ù–£ —Å—Ç–∞—Ç—å—é
        r = SCRAPER.get(f"{url}/wp-json/wp/v2/posts/{aid}?_embed", timeout=60)
        r.raise_for_status()
        return r.json()
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è ID={aid}: {e}")
        return None

def parse_and_save(post, lang, stopwords):
    time.sleep(2)
    aid, slug, link = str(post["id"]), post["slug"], post.get("link")
    
    raw_title = BeautifulSoup(post["title"]["rendered"], "html.parser").get_text(strip=True)
    title = sanitize_text(raw_title)

    if stopwords:
        for ph in stopwords:
            if ph in title.lower():
                logging.info(f"üö´ ID={aid}: –°—Ç–æ–ø-—Å–ª–æ–≤–æ '{ph}'")
                return None

    try:
        html_txt = SCRAPER.get(link, timeout=SCRAPER_TIMEOUT).text
    except Exception: return None

    meta_path = OUTPUT_DIR / f"{aid}_{slug}" / "meta.json"
    curr_hash = hashlib.sha256(html_txt.encode()).hexdigest()
    if meta_path.exists():
        try:
            m = json.loads(meta_path.read_text(encoding="utf-8"))
            if m.get("hash") == curr_hash:
                logging.info(f"‚è≠Ô∏è ID={aid}: –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π.")
                return m
        except: pass

    logging.info(f"Processing ID={aid}: {title[:30]}...")

    soup = BeautifulSoup(html_txt, "html.parser")
    
    # --- –ù–û–í–ê–Ø –ü–†–ê–í–ö–ê: –ì–õ–û–ë–ê–õ–¨–ù–ê–Ø –û–ß–ò–°–¢–ö–ê –ú–£–°–û–†–ê ---
    # –£–¥–∞–ª—è–µ–ºRelated Posts, —Ä–µ–∫–ª–∞–º–Ω—ã–µ –±–ª–æ–∫–∏ –∏ –≤–∏–¥–∂–µ—Ç—ã –î–û –Ω–∞—á–∞–ª–∞ —Å–±–æ—Ä–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫
    # –≠—Ç–æ —É–±—å–µ—Ç –±–ª–æ–∫–∏ rp4wp, –∫–æ—Ç–æ—Ä—ã–µ –ª–µ–∑–ª–∏ –≤ Lightbox –∏ –∫–æ–Ω—Ç–µ–Ω—Ç
    for garbage in soup.find_all(["div", "ul", "ol", "section", "aside"], 
                                class_=re.compile(r"rp4wp|related|ad-|post-widget-thumbnail|sharedaddy")):
        garbage.decompose()

    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ–≥–æ–≤ –∏ –ø—É—Å—Ç—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    for j in soup.find_all(["span", "script", "style", "iframe"]):
        if not hasattr(j, 'attrs') or j.attrs is None: continue 
        c = str(j.get("class", ""))
        if j.get("data-mce-type") or "mce_SELRES" in c or "widget" in c: 
            j.decompose()

    # --- –°–±–æ—Ä URL —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ü–û–†–Ø–î–ö–ê ---
    ordered_srcs = []
    seen_srcs = set()

    def add_src(url):
        if url and url not in seen_srcs:
            ordered_srcs.append(url)
            seen_srcs.add(url)

    # 1. –ü–†–ò–û–†–ò–¢–ï–¢: Featured Media (–ì–ª–∞–≤–Ω–æ–µ —Ñ–æ—Ç–æ WP –∏–∑ API)
    if "_embedded" in post and (m := post["_embedded"].get("wp:featuredmedia")):
        if isinstance(m, list) and (u := m[0].get("source_url")):
            if "logo" not in u.lower():
                add_src(u)

    # 2. –û–°–¢–ê–õ–¨–ù–´–ï: Lightbox —Å—Å—ã–ª–∫–∏ (—Ç–µ–ø–µ—Ä—å —Ç—É—Ç –Ω–µ –±—É–¥–µ—Ç –∫–∞—Ä—Ç–∏–Ω–æ–∫ –∏–∑ Related Posts)
    for link_tag in soup.find_all("a", class_="ci-lightbox", limit=10):
        if h := link_tag.get("href"): 
            if "gif" not in h.lower():
                add_src(h)

    # 3. –û–°–¢–ê–õ–¨–ù–´–ï: –ö–∞—Ä—Ç–∏–Ω–∫–∏ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ —Ç–µ–∫—Å—Ç–µ
    c_div = soup.find("div", class_="entry-content")
    if c_div:
        for img in c_div.find_all("img"):
            if u := extract_img_url(img):
                add_src(u)

    # --- –ó–∞–≥—Ä—É–∑–∫–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–Ω–¥–µ–∫—Å–æ–≤ (—á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–º–µ—à–∞–ª–∏—Å—å) ---
    images_results = [None] * len(ordered_srcs)
    if ordered_srcs:
        with ThreadPoolExecutor(5) as ex:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º—Å—è –ø–µ—Ä–≤—ã–º–∏ 10 —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ —Ñ–æ—Ç–æ
            future_to_idx = {
                ex.submit(save_image, url, OUTPUT_DIR / f"{aid}_{slug}" / "images"): i 
                for i, url in enumerate(ordered_srcs[:10])
            }
            for f in as_completed(future_to_idx):
                idx = future_to_idx[f]
                if res := f.result():
                    images_results[idx] = Path(res).name

    # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ –∫–∞–∫–æ–≥–æ-—Ç–æ —Ñ–æ—Ç–æ —Å–æ—Ä–≤–∞–ª–∞—Å—å)
    final_images = [img for img in images_results if img is not None]

    if not final_images:
        logging.warning(f"‚ö†Ô∏è ID={aid}: –ù–µ—Ç –Ω–æ—Ä–º –∫–∞—Ä—Ç–∏–Ω–æ–∫. Skip.")
        return None

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏
    paras = []
    if c_div:
        # –£–¥–∞–ª—è–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º—É—Å–æ—Ä –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ, –µ—Å–ª–∏ –æ–Ω –æ—Å—Ç–∞–ª—Å—è
        for r in c_div.find_all(["ul", "ol", "div"], class_=re.compile(r"rp4wp|related|ad-")): 
            r.decompose()
        paras = [sanitize_text(p.get_text(strip=True)) for p in c_div.find_all("p")]
    
    raw_body_text = BAD_RE.sub("", "\n\n".join(paras))

    # –û–ë–†–ê–ë–û–¢–ö–ê + –ü–ï–†–ï–í–û–î
    final_title = title
    translated_body = ""
    if lang:
        final_title, translated_body = smart_process_and_translate(title, raw_body_text, lang)
        final_title = sanitize_text(final_title)

    art_dir = OUTPUT_DIR / f"{aid}_{slug}"
    art_dir.mkdir(parents=True, exist_ok=True)
    
    (art_dir / "content.txt").write_text(raw_body_text, encoding="utf-8")
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (images[0] ‚Äî —Ç–µ–ø–µ—Ä—å –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –≥–ª–∞–≤–Ω–æ–µ —Ñ–æ—Ç–æ)
    meta = {
        "id": aid, "slug": slug, "date": post.get("date"), "link": link,
        "title": final_title, "text_file": "content.txt",
        "images": final_images, "posted": False,
        "hash": curr_hash, "translated_to": ""
    }

    if translated_body:
        (art_dir / f"content.{lang}.txt").write_text(f"{final_title}\n\n{translated_body}", encoding="utf-8")
        meta.update({"translated_to": lang, "text_file": f"content.{lang}.txt"})

    with open(meta_path, "w", encoding="utf-8") as f: 
        json.dump(meta, f, ensure_ascii=False, indent=2)
    return meta

# --- MAIN ---
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--base-url", required=True)
    parser.add_argument("--slug", default="national")
    parser.add_argument("-n", "--limit", type=int, default=10)
    parser.add_argument("-l", "--lang", default="ru")
    parser.add_argument("--posted-state-file", default="articles/posted.json")
    parser.add_argument("--stopwords-file", default="stopwords.txt")
    args = parser.parse_args()

    try:
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –ø–∞–ø–∫–∏ —Å—Ç–∞—Ç–µ–π, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 100 –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö
        cleanup_old_articles(Path(args.posted_state_file), OUTPUT_DIR)
        
        # –ü–æ–ª—É—á–∞–µ–º ID –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        cid = fetch_cat_id(args.base_url, args.slug)
        
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ: –ª–µ–≥–∫–∏–π —Å–ø–∏—Å–æ–∫ ID, –∏—Å—Ç–æ—Ä–∏—é –∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        posts_light = fetch_posts_light(args.base_url, cid, FETCH_DEPTH)
        posted = load_posted_ids(Path(args.posted_state_file))
        stop = load_stopwords(Path(args.stopwords_file))
        
        # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â–∏–π –∫–∞—Ç–∞–ª–æ–≥ –∏–∑ —Ñ–∞–π–ª–∞
        catalog = []
        if CATALOG_PATH.exists():
            try:
                with open(CATALOG_PATH, 'r', encoding='utf-8') as f:
                    catalog = json.load(f)
            except Exception:
                logging.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–∞—Ç–∞–ª–æ–≥. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π.")

        new_metas = []
        count = 0
        
        # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏
        for p_short in posts_light:
            if count >= args.limit:
                break
            
            aid = str(p_short["id"])
            if aid in posted:
                continue # –≠—Ç—É —Å—Ç–∞—Ç—å—é —É–∂–µ –ø–æ—Å—Ç–∏–ª–∏, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            
            logging.info(f"üÜï –ù–∞–π–¥–µ–Ω–∞ –Ω–æ–≤–∞—è —Å—Ç–∞—Ç—å—è ID={aid}. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–µ—Ç–∞–ª–∏...")
            full_post = fetch_single_post_full(args.base_url, aid)
            
            if full_post:
                # parse_and_save –≤–Ω—É—Ç—Ä–∏ —Å–µ–±—è –¥–µ–ª–∞–µ—Ç AI-—á–∏—Å—Ç–∫—É –∏ –ø–µ—Ä–µ–≤–æ–¥
                if meta := parse_and_save(full_post, args.lang, stop):
                    new_metas.append(meta)
                    count += 1

        # 3. –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞ –∏ –æ—Ç—á–µ—Ç –¥–ª—è GitHub Actions
        if new_metas:
            # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º –¥—É–±–ª–∏ –≤ –∫–∞—Ç–∞–ª–æ–≥–µ: —É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ —Å —Ç–µ–º–∏ –∂–µ ID
            new_ids = {str(m["id"]) for m in new_metas}
            catalog = [item for item in catalog if str(item.get("id")) not in new_ids]
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤–µ–∂–µ–ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            catalog.extend(new_metas)
            
            with open(CATALOG_PATH, "w", encoding="utf-8") as f:
                json.dump(catalog, f, ensure_ascii=False, indent=2)
            
            # –≠—Ç–æ —Å–∏–≥–Ω–∞–ª –¥–ª—è GitHub Actions, —á—Ç–æ –Ω—É–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –ø–æ—Å—Ç–µ—Ä
            print("NEW_ARTICLES_STATUS:true")
            logging.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –î–æ–±–∞–≤–ª–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(new_metas)}")
        else:
            print("NEW_ARTICLES_STATUS:false")
            logging.info("üîç –ù–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

    except Exception:
        logging.exception("üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ main:")
        exit(1)

if __name__ == "__main__":
    main()
