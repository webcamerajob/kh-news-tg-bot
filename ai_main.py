import os
import sys
import json
import logging
import time
import requests
import translators as ts  # –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞
import main  # –¢–≤–æ–π –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π main.py

# --- –°–ü–ò–°–û–ö –ú–û–î–ï–õ–ï–ô ---
AI_MODELS = [
    "meta-llama/llama-3.3-70b-instruct:free",      # –û—Ç–ª–∏—á–Ω–æ –ø–æ–Ω–∏–º–∞–µ—Ç –∑–∞–ø—Ä–µ—Ç—ã
    "google/gemini-2.0-flash-exp:free",
    "deepseek/deepseek-r1-distill-llama-70b:free",
    "meta-llama/llama-3.2-3b-instruct:free",
]

OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")

# --- –û–ë–´–ß–ù–´–ô –ü–ï–†–ï–í–û–î ---
def standard_translate(text: str, to_lang: str = "ru") -> str:
    if not text: return ""
    providers = ["google", "bing", "yandex"]
    for provider in providers:
        try:
            time.sleep(1) 
            result = ts.translate_text(
                query_text=text,
                translator=provider,
                from_language="en",
                to_language=to_lang,
                timeout=20
            )
            return result
        except Exception: continue
            
    logging.error("‚ùå –í—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –ø–µ—Ä–µ–≤–æ–¥–∞ –æ—Ç–∫–∞–∑–∞–ª–∏.")
    return text

# --- –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–ï ---
def format_paragraphs(text: str) -> str:
    paragraphs = [p.strip() for p in text.replace('\r', '').split('\n') if p.strip()]
    return "\n\n".join(paragraphs)

# --- –£–î–ê–õ–ï–ù–ò–ï –í–°–¢–£–ü–õ–ï–ù–ò–ô (–ü–û–°–¢-–û–ë–†–ê–ë–û–¢–ö–ê) ---
def strip_ai_chatter(text: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç —Ç–∏–ø–∏—á–Ω—ã–π –º—É—Å–æ—Ä, –µ—Å–ª–∏ –ò–ò –≤—Å–µ-—Ç–∞–∫–∏ –æ—Å–ª—É—à–∞–ª—Å—è."""
    bad_prefixes = [
        "Here is a summary", "Here is the summary", "In this article", 
        "The article discusses", "According to the report", "Summary:",
        "–í–æ—Ç –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ", "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –æ —Ç–æ–º", "–†–µ–∑—é–º–µ:"
    ]
    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –º—É—Å–æ—Ä–∞, –∏—â–µ–º –ø–µ—Ä–≤–æ–µ –¥–≤–æ–µ—Ç–æ—á–∏–µ –∏–ª–∏ –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫—É
    for prefix in bad_prefixes:
        if text.lower().startswith(prefix.lower()):
            # –ü—Ä–æ–±—É–µ–º –æ–±—Ä–µ–∑–∞—Ç—å –ø–æ –¥–≤–æ–µ—Ç–æ—á–∏—é (Here is the summary: News...)
            parts = text.split(':', 1)
            if len(parts) > 1:
                return parts[1].strip()
            # –ò–ª–∏ –ø—Ä–æ—Å—Ç–æ –≤—ã–∫–∏–¥—ã–≤–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É
            parts = text.split('\n', 1)
            if len(parts) > 1:
                return parts[1].strip()
    return text

# --- –ì–õ–ê–í–ù–ê–Ø –õ–û–ì–ò–ö–ê ---
def ai_clean_and_then_translate(text: str, to_lang: str = "ru", provider: str = "ai") -> str:
    if not text or not text.strip(): return ""
    
    if not OPENROUTER_API_KEY: 
        logging.warning("‚ö†Ô∏è [AI] –ö–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥.")
        return standard_translate(text, to_lang)

    logging.info("‚è≥ –ü–∞—É–∑–∞ 5 —Å–µ–∫ –ø–µ—Ä–µ–¥ –ò–ò...")
    time.sleep(5) 
    logging.info(f"ü§ñ [AI] –ß–∏—Å—Ç–∫–∞ (Strict Mode)...")

    # üî• –ñ–ï–°–¢–ö–ò–ô –ü–†–û–ú–ü–¢ üî•
    prompt = (
        f"You are a backend news processor API.\n"
        f"INPUT: Raw news text with ads and noise.\n"
        f"OUTPUT: A clean, concise summary in ENGLISH.\n\n"
        "STRICT NEGATIVE CONSTRAINTS (DO NOT IGNORE):\n"
        "1. NO INTRODUCTIONS (Never write 'Here is a summary', 'The text says', etc.).\n"
        "2. NO OUTROS (No 'Hope this helps').\n"
        "3. NO LABELS (Do not write 'Summary:' or 'Headline:').\n"
        "4. NO META-TALK. Start directly with the first word of the news story.\n\n"
        "CONTENT RULES:\n"
        "- Remove ads, links, and 'Related Articles'.\n"
        "- Keep dates, names, and locations exact.\n"
        "- Use neutral, journalistic tone.\n\n"
        f"RAW TEXT:\n{text[:15000]}"
    )

    clean_english_text = ""

    for model in AI_MODELS:
        try:
            response = requests.post(
                url="https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
                    "Content-Type": "application/json",
                    "HTTP-Referer": "https://github.com/parser-bot",
                    "X-Title": "NewsBot",
                },
                data=json.dumps({
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.2 # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ = –º–µ–Ω—å—à–µ –æ—Ç—Å–µ–±—è—Ç–∏–Ω—ã
                }),
                timeout=55
            )

            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and result['choices']:
                    clean_english_text = result['choices'][0]['message']['content'].strip()
                    logging.info(f"‚úÖ [AI] –û—á–∏—Å—Ç–∫–∞ —É—Å–ø–µ—à–Ω–∞ ({model}).")
                    break
            elif response.status_code == 429:
                time.sleep(2)
            else:
                logging.warning(f"‚ö†Ô∏è [AI] {model} –æ—à–∏–±–∫–∞ {response.status_code}.")
        
        except Exception: continue

    if not clean_english_text:
        logging.error("‚ùå [AI] –°–±–æ–π. –ü–µ—Ä–µ–≤–æ–¥–∏–º –æ—Ä–∏–≥–∏–Ω–∞–ª.")
        clean_english_text = text

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç—Ä–∞—Ö–æ–≤–∫–∞: —á–∏—Å—Ç–∏–º –º—É—Å–æ—Ä–Ω—ã–µ —Ñ—Ä–∞–∑—ã –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ
    clean_english_text = strip_ai_chatter(clean_english_text)

    # 2. –ü–ï–†–ï–í–û–î
    logging.info(f"üåç [Translators] –ü–µ—Ä–µ–≤–æ–¥ —á–∏—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞...")
    final_russian_text = standard_translate(clean_english_text, to_lang)
    
    return format_paragraphs(final_russian_text)

if __name__ == "__main__":
    main.translate_text = ai_clean_and_then_translate
    main.main()
